{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Gradient Descent Algorithm from First Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Charlie Bailey (peba2926)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"What I cannot create, I do not understand..\"\n",
    "\n",
    "—**Richard Feynman**\n",
    "\n",
    "\"The back-propagation algorithm is very simple. To compute the gradient of some scalar $z$ with respect to one of its ancestors $x$ in the graph, we begin by observing that the gradient with respect to $z$ is given by $\\frac{dz}{dz} = 1$. We can then compute the gradient with respect to each parent of $z$ in the graph by multiplying the current gradient by the Jacobian of the operation that produced $z$.\"\n",
    "\n",
    "—**Deep Learning** by Ian Goodfellow, Yoshua Bengio, and Aaron Courville."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "While the paragraph above may seem \"very simple\" to the leading minds in field of Artificial Intelligence, for those new to the field, even the fundamental concept of a gradient can be confusing. What exactly is a `gradient`? How do we actually go about finding one? And once we have this gradient, how is it actually used?\n",
    "\n",
    "Answering these questions will take us to the heart of modern Artificial Intelligence, where methods like `gradient descent` power the training of everything from simple regression models to advanced neural networks. Many people—myself included—are introduced to gradient descent through code libraries and black-box optimization routines. Without a deeper understanding though, these training methods can seem like black magic rather than a systematic mathematical procedure.\n",
    "\n",
    "The goal of this project is to demystify gradient descent by building it from the ground up—without relying on any pre-built functions or libraries. Guided by [Callahmn and Hoffman's](https://www.amazon.com/Calculus-Context-James-Callahan/dp/0716726300) computation-based approach to understanding calculus we will start with the foundational calculus concepts underlying gradients and then, building from first principles, we will create a custom implementation of the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Nine: Functions of Several Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 9.1: Graphs and Level Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can understand what exactly a gradient is, we first need to understand the geometry of functions with two or more variables. In reality, AI models may have thousands, millions or even billions of variables (also known as parameters), so it is impossible to truly visualize what the gradient would \"look like\" in these high-dimensional spaces. Luckily for us though, the concept of a gradient in two dimensions extends well enough to these high-dimensional spaces to allow us to get a foot hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1:\n",
    "\n",
    "Graph the function $z = sin(x) sin(y)$ on the domain  $0 \\leq x \\leq 2\\pi, 0 \\leq y \\leq 2\\pi$.\n",
    "\n",
    "How many maximum points do you see?\n",
    "\n",
    "How many minimum points?\n",
    "\n",
    "How many saddles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points for plotting\n",
    "two_pi = 2*np.pi\n",
    "x = np.linspace(0, two_pi, 1000)\n",
    "y = np.linspace(0, two_pi, 1000)\n",
    "\n",
    "# create matrices for plotting\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# calculate z-points\n",
    "Z = np.sin(X) * np.sin(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate plot\n",
    "fig1 = go.Figure(data=[go.Surface(x=X, y=Y, z=Z,)])\n",
    "fig1.update_layout(\n",
    "    title='Graph of z = sin(x)sin(y)',\n",
    "    scene=dict(\n",
    "        xaxis_title='X axis',\n",
    "        yaxis_title='Y axis',\n",
    "        zaxis_title='Z axis',\n",
    "        camera=dict(\n",
    "            up=dict(x=0, y=0, z=1),\n",
    "            center=dict(x=0, y=0, z=0.25),\n",
    "            eye=dict(x=2, y=2, z=1.5)\n",
    "        )\n",
    "    ),\n",
    "    width=800,\n",
    "    height=800\n",
    ")\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: if you are accessing this project in PDF format, this is a 3D interactive graph that unfortunately won't render to PDF. This project is best viewed in it's Jupyter Notebook format to get the full benefit of interacting with the graph.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can \"walk around\" this graph by clicking and dragging to form different perspectives. We can also hover our cursor over various points in the graph to see the three coordinates at any location.\n",
    "\n",
    "By moving our cursor to inspect the various `critical points`, we can answer the questions above:\n",
    "\n",
    "**Maximums: 2**\n",
    "\n",
    "| | Point 1 | Point 2 | \n",
    "|-|---------|---------|\n",
    "| x | 1.56 | 4.72 |\n",
    "| y| 1.56 | 4.72 |\n",
    "| z | 1 | 1 |\n",
    "\n",
    "**Minimums: 2**\n",
    "\n",
    "| | Point 1 | Point 2 | \n",
    "|-|---------|---------|\n",
    "| x | 4.72 | 1.56 |\n",
    "| y| 1.56 | 4.72 |\n",
    "| z | -1 | -1 |\n",
    "\n",
    "**Saddle Points: 1**\n",
    "\n",
    "| | Point 1 | \n",
    "|-|---------|\n",
    "| x | 3.14 |\n",
    "| y| 3.14 |\n",
    "| z | 9.89 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth pausing for a moment to realize that what we just did—specifically using hovering our cursor to find the lowest $z$ values—is **exactly** how the gradient descent algorithm works!\n",
    "\n",
    "What we just did visually with a cursor is what the gradient descent algorithm needs to do to minimize an `objective function`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "***\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so conceptually we know where we need to go. Bring the cursor to the lowest point on the graph. Simple enough. But how did we know exactly what to do? Even more so, how would we tell a computer to do what we just did? How do we explain to a computer the concept of \"move to the lowest point\" on a graph?\n",
    "\n",
    "If you scroll over the graph again you will notice that there are several gray lines that appear as you move your cursor around. The horizontal lines are called the `contour` lines and the vertical lines are the `grid` lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('assets/contour_grid_lines.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally, the contour of $z = f(x,y)$ is the set of points in the $x, y$ plane where $f$ has some fixed value:\n",
    "$$f(x,y) = c$$\n",
    "This fixed value $c$ is called the `level`.\n",
    "\n",
    "With these concepts in mind, we can use the metaphor of pressing a piece of paper \"down\" through the graph to better understand the contour lines. Imagine this piece of paper being connected to your cursor as you drag it down the graph toward the minimum. The paper stays flat and perpendicular to the $z$-axis as you move the cursor. At each step of the way, all of the points that the paper is currently touching have the same $c$ value. And at each step of the way, this $c$ value is smaller than the value at the step above. Eventually, we reach the bottom of the bowl and the paper is just barely touching the bottom of the graph at a single point.\n",
    "\n",
    "More formally, we would say that this piece of paper—or *the plane at contour level $c$*—is `tangent` to the function $f(x,y)$ at points $(a,b)$. This point $(a, b, f(a,b))$ is one of the `minimums` of the graph. Here we can recognize that a horizontal tangent to a function indicates that the instantaneous rate of change at that point is zero. Said another way, the `derivative` of our function at the lowest possible contour level still touching the graph is zero.\n",
    "\n",
    "Now this is sounding a bit more like something we could program into a computer. If we were able to somehow follow these contour lines down—we would be able to navigate to the minimum.\n",
    "\n",
    "Let's examine in a bit more detail what exactly is going on with these changing contour lines to figure out how we might tell the computer to \"bring the cursor to the lowest point on the graph.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 9.2: Local Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next part, we will be looking at the graph of $f(x,y) = x^3 - 4x -y^2$.\n",
    "\n",
    "Let's take a look at what life is like down on the surface of our function. Using the \"microscope\" metaphor, we will zoom in on the point $(x, y) = (1.8, -1)$ to examine the `local linearity` of this function—which is the idea that the graph approaches a flat plane when viewed under intense magnification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points for a new graph\n",
    "x_1 = np.linspace(-1.5, 4.5, 1000)\n",
    "y_1 = np.linspace(-4, 2, 1000)\n",
    "\n",
    "# create matrices for plotting\n",
    "X_1, Y_1 = np.meshgrid(x_1, y_1)\n",
    "\n",
    "# calculate z-points\n",
    "Z_1 = X_1**3 - 4*X_1 - Y_1**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the plot to show contour lines and new function\n",
    "# generate plot\n",
    "fig2 = go.Figure(data=[go.Surface(x=X_1, y=Y_1, z=Z_1)])\n",
    "fig2.update_traces(\n",
    "    contours=dict(\n",
    "        x=dict(show=True, start=X_1.min(), end=X_1.max(), size=0.3, color='black', width=1),\n",
    "        y=dict(show=True, start=Y_1.min(), end=Y_1.max(), size=0.3, color='black', width=1),\n",
    "        z=dict(show=True, start=Z_1.min(), end=Z_1.max(), size=0.3, color='black', width=1)\n",
    "    )\n",
    ")\n",
    "fig2.update_layout(\n",
    "    title='Graph of z = x^3 - 4x -y^2',\n",
    "    scene=dict(\n",
    "        xaxis_title='X axis',\n",
    "        yaxis_title='Y axis',\n",
    "        zaxis_title='Z axis',\n",
    "        camera=dict(\n",
    "            up=dict(x=0, y=0, z=1),\n",
    "            center=dict(x=0, y=0, z=0.25),\n",
    "            eye=dict(x=0, y=-2, z=1.75)\n",
    "        )\n",
    "    ), \n",
    "    width=800,\n",
    "    height=800\n",
    ")\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2:\n",
    "\n",
    "Graph the function $z = x^3 -4x - y^2$ on a domain centered at th  point $(x,y) = (1.8, -1)$ and magnify until it looks like a plane.\n",
    "\n",
    "Use the following steps to estimate the rate of change $\\Delta z / \\Delta x$ at $(x,y) = (1.8, -1)$:\n",
    "\n",
    "1) What is the horizontal spacing between the two contours closest to the point $(1.8, -1)$?\n",
    "2) Find the z-levels $z_1$ and $z_2$ of those contours and then compute $\\Delta z = z_2 - z_1$.\n",
    "3) Finally, calculate the value $\\Delta z / \\Delta x$.\n",
    "\n",
    "Finally, repeat these steps to find to estimate the rate of change $\\Delta z / \\Delta y$ at $(x,y) = (1.8, -1)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('assets/zoom_1.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('assets/zoom_2.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('assets/zoom_3.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the Plotly zoom capabilities are not robust enough to get a perfect locally linear plane, but we can get pretty close around our desired $(x,y)$ coordinate using some manual zooming on the image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('assets/zoom_x_work3.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we are in flatland! The little red dot here is our cursor. Let's use the steps above to calculate $\\Delta z / \\Delta x$.\n",
    "\n",
    "1) Calculate $x_2 - x_1$ which is $1.87 - 1.71 = 0.16$\n",
    "2) Calculate $z_2 - z_1$ which is $-1.94 - (-2.84) = 0.9$\n",
    "3) Therefore, $\\Delta z / \\Delta x$ is $0.9/0.16 = 5.625$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('assets/zoom_y_work4.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same steps above to calculate $\\Delta z / \\Delta y$\n",
    "\n",
    "1) Calculate $y_2 - y_1$ which is $-1.25 - (-0.79) = -0.46$\n",
    "2) Calculate $z_2 - z_1$ which is $-2.85 - (-1.94) = -0.91$\n",
    "3) Therefore, $\\Delta z / \\Delta y$ is $-0.91 / (-0.46) \\approx 1.98$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with these two calculations, we can now introduce the concept of the `gradient`.\n",
    "\n",
    "**The gradient of a function $f = f(x,y)$ is the vector whose components are its partial rates of change:**\n",
    "$$grad f = \\nabla f = (f_x, f_y) = (\\frac{\\partial z}{\\partial x}, \\frac{\\partial z}{\\partial y})$$\n",
    "\n",
    "This means that, based on our numerical approximation using the contours of the locally linear plane, the gradient of the function $z = f(x,y) = x^3 - 4x -y^2$ at the point $(x, y) = (1.8, -1)$ is the vector:\n",
    "$$\\nabla z = (5.625, 1.98)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: Coding the Gradient Descent Algorithm\n",
    "\n",
    "We now have nearly all the pieces we need to be able to tell a computer how to find the minimum of a function. Let's pull in some of the numerical point derivative work we did earlier in the course to see if we can automate the gradient finding process we just went through by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for reference: numerical point derivative of a single variable function\n",
    "def numerical_point_derivative(f, a, h=0.0001):\n",
    "    return (f(a + h) - f(a - h)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrapolation to a partial derivatives\n",
    "def calc_partial_deriv_x(f, x, y, h=0.05):\n",
    "    x_1 = x+h\n",
    "    x_2 = x-h\n",
    "    z_1 = f(x_1, y)\n",
    "    z_2 = f(x_2, y)\n",
    "    return ((z_2 - z_1) / (x_2 - x_1))\n",
    "\n",
    "def calc_partial_deriv_y(f, x, y, h=0.05):\n",
    "    y_1 = y+h\n",
    "    y_2 = y-h\n",
    "    z_1 = f(x, y_1)\n",
    "    z_2 = f(x, y_2)\n",
    "    return ((z_2 - z_1) / (y_2 - y_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on points used in hand-worked example\n",
    "def func(x, y):\n",
    "    return x**3 - 4*x - y**2\n",
    "\n",
    "dz_x = calc_partial_deriv_x(func, x=1.8, y=-1)\n",
    "dz_y = calc_partial_deriv_y(func, x=1.8, y=-1)\n",
    "print(dz_x)\n",
    "print(dz_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Using similar $h$ values to the ones provided by the contours in the locally linear plane, we are now able to programmatically get the partial derivatives of both $x$ and $y$. As noted above, these are the components of our gradient vector $\\nabla z$. This is our compass telling us the direction of `steepest ascent`—that if we went $\\nabla z_x$ units in the x-direction and then $\\nabla z_y$ units in the y-direction, this would put us at an $(x,y)$ point with the highest value of $z$.\n",
    "\n",
    "So, we can logically deduce that if this gradient vector is pointing in the direction of steepest ascent, all we need to do to find our minimum is head in the opposite direction.\n",
    "\n",
    "There's just one last piece of information we need to be able to implement our gradient descent algorithm. This is the idea of a `learning rate`. Since our gradient can be very steep in certain places, this means that taking a \"full unit step\" in terms of following the vector displacement may cause us to overshoot our minimum by taking too large of a step. Therefore, we update our location by a small proportion of the full displacement calculated by our gradient vector. For this implementation, we'll set this learning rate to be $0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(f, x, y):\n",
    "    # better approximation with smaller h value\n",
    "    dz_x = calc_partial_deriv_x(f, x, y, h=0.0001)\n",
    "    dz_y = calc_partial_deriv_y(f, x, y, h=0.0001)\n",
    "    return (dz_x, dz_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, x_init, y_init, alpha=0.1):\n",
    "    # store path for plotting\n",
    "    x_points = [x_init]\n",
    "    y_points = [y_init]\n",
    "    z_points = [f(x_init, y_init)]\n",
    "\n",
    "    # initialize grad and displacements\n",
    "    init_grad_z = calc_grad(f, x_init, y_init)\n",
    "    step_x = x_init - (alpha*init_grad_z[0])\n",
    "    step_y = y_init - (alpha*init_grad_z[1])\n",
    "    delta_z = f(step_x, step_y)\n",
    "    \n",
    "\n",
    "    # iterate until we can't descend any further\n",
    "    while (delta_z < z_points[-1]):\n",
    "        # store previous step vals\n",
    "        x_points.append(step_x)\n",
    "        y_points.append(step_y)\n",
    "        z_points.append(delta_z)\n",
    "\n",
    "        # calculate new grad and next steps\n",
    "        new_grad_z = calc_grad(f, step_x, step_y)\n",
    "        next_step_x = step_x - (alpha*new_grad_z[0])\n",
    "        next_step_y = step_y - (alpha*new_grad_z[1])\n",
    "\n",
    "        # update steps\n",
    "        delta_z = f(next_step_x, next_step_y)\n",
    "        step_x = next_step_x\n",
    "        step_y = next_step_y\n",
    "\n",
    "    # store final step vals\n",
    "    x_points.append(step_x)\n",
    "    y_points.append(step_y)\n",
    "    z_points.append(delta_z)\n",
    "    \n",
    "    return (x_points, y_points, z_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on original function at x = 1.66, y = 3.37\n",
    "x_path, y_path, z_path = gradient_descent(lambda x, y: np.sin(x) * np.sin(y), 1.66, 3.37)\n",
    "print('x::: ', x_path[-1])\n",
    "print('y::: ', y_path[-1])\n",
    "print('z::: ', z_path[-1])\n",
    "print('steps:::', len(z_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HUZZAH!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare these values against the values we found using our manual inspection above and see that we have indeed found one of the minimum points of the function!\n",
    "\n",
    "**Minimums:**\n",
    "\n",
    "| | Point 1 | Point 2 | GD |\n",
    "|-|---------|---------|----|\n",
    "| x | 4.72 | 1.56 | **1.57** |\n",
    "| y| 1.56 | 4.72 | **4.71** |\n",
    "| z | -1 | -1 | **-0.99999** |\n",
    "\n",
    "Let's plot the path our computed cursor took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points for plotting\n",
    "two_pi = 2*np.pi\n",
    "x = np.linspace(0, two_pi, 1000)\n",
    "y = np.linspace(0, two_pi, 1000)\n",
    "\n",
    "# create matrices for plotting\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# calculate z-points\n",
    "Z = np.sin(X) * np.sin(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate plot\n",
    "fig3 = go.Figure(data=[go.Surface(x=X, y=Y, z=Z,)])\n",
    "fig3.update_layout(\n",
    "    title='Graph of z = sin(x)sin(y)',\n",
    "    scene=dict(\n",
    "        xaxis_title='X axis',\n",
    "        yaxis_title='Y axis',\n",
    "        zaxis_title='Z axis',\n",
    "        camera=dict(\n",
    "            up=dict(x=0, y=0, z=1),\n",
    "            center=dict(x=0, y=0, z=0.25),\n",
    "            eye=dict(x=2, y=2, z=1.5)\n",
    "        )\n",
    "    ),\n",
    "    width=800,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# create gradient descent path using stored points\n",
    "path_trace = go.Scatter3d(\n",
    "    x=x_path,\n",
    "    y=y_path,\n",
    "    z=z_path,\n",
    "    mode='lines',\n",
    "    line=dict(\n",
    "        color='red',\n",
    "        width=5\n",
    "    ),\n",
    "    name='Steepest Descent Path'\n",
    ")\n",
    "\n",
    "# add path \n",
    "fig3.add_trace(path_trace)\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "So there we have it—gradient descent from the ground up! By working from the first principles, we have (hopefully) gained a deeper intuition for how this algorithm works and the calculus principles that power it's operation.\n",
    "\n",
    "Thank you for joining me on this journey!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Python 3.10.16 (.venv)",
   "language": "python",
   "name": "ml-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
